---
title: |
  | **Optimizations &**
  | **Genetic Algorithms**
author: |
  | ![](./figures/Mozhan Soltani.jpg){width=12%}
  | Mozhan Soltani
  | PhD student 
  | Software Engineering Research Group
date: "`r Sys.Date()`"
output: revealjs::revealjs_presentation
csl: ./../ieee.csl
---

```{r child="./../header.Rmd", include=FALSE}
```

## **Optimizations**


![](./figures/time.svg){width=70%}


## **What Is Optimization?**
![](./figures/function.svg){width=45%}

- Optimization is the process of adjusting the **inputs** to or
  **characteristics** of a device, mathematical process, or experiment to find
  the minimum or maximum output.
- The input consists of variables; the process or function is known as the
  _cost/objective_ function; and the output is the _cost/fitness_.
  

## **Trial and Error Optimization**

refers to the process of adjusting variables that affect the output without
knowing much about the process that produces the output.

e.g., adjusting the antenna to get the best reception.

![](./figures/TV.svg){width=45%}


## **Dynamic vs. Static Optimization**
Dynamic optimization is function of time!

e.g., The fastest way to work may depend on the time of day and weather.

![](./figures/routes.svg){width=45%}

## **Descrete or Continuous Optimization**

**Discrete** variables have only a finite number of possible values, whereas
**continuous** variables have an infinite number of possible values.

e.g.: If we are deciding in what order to attack a series of tasks on a list,
discrete optimization is employed.

![Continuous function](./figures/continuous.svg){width=45%}

## **Natural Optimization Methods**

- Simulated Annealing
- Particle swarm optimization
- Ant colony optimization
- Evolutionary algorithms
- Genetic Algorithm
- etc.

![](./figures/ant.svg){width=50%}

## **Simulated Annealing**
Simulates the annealing process:     
![](./figures/simulated.svg){width=45%}

. . . 

a substance is heated above its melting temperature and then gradually cooled to
produce the crystalline lattice, which minimizes its energy probability
distribution.


## **Simulated Annealing**
So lowering the temperature until the material is in a steady _frozen_ state can
be preceived as searching for the temperature and thereby material state in
which the internal particles have the minimum energy.

![](./figures/searchtemp.svg){width=40%}

## **Simulated Annealing (SA) Algorithm**
```{}    
1. Set temperature T to maximum
2. Generate a random solution
3. Calculate fitness/cost
4. Generate a neighbouring solution
5. Calculate the fitness/cost of the new solution
6. Compare f(new) and f(old)  
``` 
```{python, eval=FALSE} 
  if  f(new) < f(old) then pick the new solution!
  else use an acceptance rate to choose between the two!   
    ar = e^[(|f(new) - f(old)|)/T], (where e = 2.7182)   
    if ar > random, then pick the new solution! (where 0<random<1)
```
		
```{} 
7. Reset T: T = T * alpha (where 0.8<alpha<0.99)
8. Repeat 4-7 until:   
		either the global optimum is found  
		or T reaches the minimum.
```

## **SA in Python** ##

Individual solutions and fitness/cost function would depend on the optimization
problem of course!   

```{python, echo = TRUE, fig.width=4, fig.height=6}
import random
import math

def anneal(solution):
    old_cost = cost(solution)
    T = 1.0
    T_min = 0.00001
    alpha = 0.9
    while T > T_min:
            new_solution = neighbor(solution)
            new_cost = cost(new_solution)
            ap = acceptance_probability(old_cost, new_cost, T)
            if ap > random():
                solution = new_solution
                old_cost = new_cost
    T = T*alpha
    return solution, old_cost

def acceptance_probability(prev_score,next_score,temperature):
    return math.exp(abs(next_score-prev_score)/temperature)

def neighbor(solution):
    return solution.neighbor

def cost(solution):
    return solution.fitness
```


## **Hill Climbing**

Very similar to simulated annealing!   
The difference is that worse neighbours will never get to be selected!

![](./figures/hillclimbing.svg){width=50%}

## **Local and Global Optimum**

If hill climbing hits the local optimum, one approach is to restart the search!

![](./figures/localoptimum.svg){width=55%}

## **Genetic Algorithms**
Inspired by the _Darwinian evolution_ and the concept of **survival of the
fittest**!
![](./figures/evolution.svg){width=75%}

## **Genetic Algorithm Overview**
Each point in the search space is an referred to as an _individual_
or _chromosome_.    

Individuals collectively form a _population_, which is _iteratively evolved_
until the optimum is found or the search budget is exhausted.

![](./figures/GA loop.svg){width=75%}

## **Evolutionary Operators in GA**      
    
**Crossover** is used to _re-combine_ the _properties_ from parents.    
**Mutation** is applied to explore the _neighborhood_!

![](./figures/operators.svg){width=75%}

## **Let's play a guessing game!**

![](./figures/guessing.svg){width=75%}

## **What do we need to implement?**
1. Individual (_chromosome_) generator
2. Fitness function
3. Evolutionary operators (_only mutation for now_)
4. Search engine
5. Display function, showing the search progress (optional!) 

## **Individual Generator and Fitness Function**
```{python, eval=FALSE}
class Chromosome:
    def __init__(self, genes, fitness):
        self.Genes = genes
        self.Fitness = fitness
```
. . .     

```{python, eval=FALSE}
def generate_parent(length, geneSet, get_fitness):
    genes = []
    while len(genes) < length:
        sampleSize = min(length - len(genes), len(geneSet))
        genes.extend(random.sample(geneSet, sampleSize))
    genes = ''.join(genes)
    fitness = get_fitness(genes)
    return Chromosome(genes, fitness)
```
. . .     

```{python, eval=FALSE}  
def get_fitness(guess, target):
    return sum(1 for expected, actual in zip(target, guess)
               if expected == actual)
```

## **Mutation and Search Engine**
```{python, eval=FALSE}
def mutate(parent, geneSet, get_fitness):
    index = random.randrange(0, len(parent.Genes))
    childGenes = list(parent.Genes)
    newGene, alternate = random.sample(geneSet, 2)
    childGenes[index] = alternate if newGene == childGenes[index] 
                                  else newGene
    genes = ''.join(childGenes)
    fitness = get_fitness(genes)
    return Chromosome(genes, fitness)
```
. . .     

```{python, eval=FALSE}
def get_best(get_fitness, targetLen, optimalFitness, geneSet, display):
    random.seed()
    bestParent = _generate_parent(targetLen, geneSet, get_fitness)
    display(bestParent)
    if bestParent.Fitness >= optimalFitness:
        return bestParent
    while True:
        child = _mutate(bestParent, geneSet, get_fitness)
        if bestParent.Fitness >= child.Fitness:
            continue
        display(child)
        if child.Fitness >= optimalFitness:
            return child
        bestParent = child
```

## **Let's test it!**
```{python, eval=FALSE}
class GuessPasswordTest(unittest.TestCase):
    geneset = " abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!.,"

    def test_Hello_World(self):
        target = "Hello World!"
        self.guess_password(target)
    
    def test_For_I_am_fearfully_and_wonderfully_made(self):
        target = "For I am fearfully and wonderfully made."
        self.guess_password(target)

    def guess_password(self, target):
        startTime = datetime.datetime.now()

        def fnGetFitness(genes):
            return genetic.get_fitness(genes, target)

        def fnDisplay(candidate):
            genetic.display(candidate, startTime)

        optimalFitness = len(target)
        best = genetic.get_best(fnGetFitness, len(target), optimalFitness,
                                self.geneset, fnDisplay)
        self.assertEqual(best.Genes, target)
```

## **Travelling Salesman**

To further practice, try implementing GA for the TS problem!

![](./figures/salesman.svg){width=75%}

## **Which search algorithm to use?**
**No Free Lunch Theorem** says that the average performance of all search
algorithms on all optimization problems is equal!

_i.e._ GA performs no better than pure random search when applied to all
problems!

![](./figures/freelunch.svg){width=40%}

## **So again - which search algorithm to use?**
We can always start by simpler algorithms, e.g. hill climbing, and see if they
are good enough for the problem at hand.

If the search landscape is perturbated, more advanced algorithms which can
effectively explore the landscape may be a better start.

In any case, **fitness/cost function** and **individual representation** are
the two key elements to prepare when applying any search algorithm.


## **REFERENCES**

* Python examples:     
https://github.com/handcraftsman/GeneticAlgorithmsWithPython
 

## Copyright
![Creative Commons](https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png) This
work is (c) 2017 - onwards by TU Delft and Mozhan Soltani and licensed under the
[Creative Commons Attribution-NonCommercial-ShareAlike 4.0
International](http://creativecommons.org/licenses/by-nc-sa/4.0/) license.

<!--html_preserve-->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-2510585-3', 'auto');
  ga('send', 'pageview');

</script>
<!--/html_preserve-->
