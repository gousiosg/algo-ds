---
title: |
  | **Optimizations &**
  | **Genetic Algorithms**
author: |
  | Mozhan Soltani
  | PhD student 
  | Software Engineering Research Group
date: "`r Sys.Date()`"
output: revealjs::revealjs_presentation
csl: ./../ieee.csl
---

```{r child="./../header.Rmd", include=FALSE}
```

## **What is optimization about?**


![](./figures/time.svg){width=70%}


## **What Is Optimization?**
![](./figures/function.svg){width=45%}

- Optimization is the process of adjusting the **inputs** to or
  **characteristics** of a device, mathematical process, or experiment to find
  the minimum or maximum output.      
  e.g., adjusting the screen light so it takes as little power as possible to
  produce sufficient light!
  

## **Trial and Error Optimization**

refers to the process of adjusting variables that affect the output without
knowing much about the process that produces the output.

e.g., adjusting the antenna to get the best reception.

![](./figures/TV.svg){width=45%}


## **Dynamic vs. Static Optimization**
Dynamic optimization is function of time!

e.g., The fastest way to work may depend on the time of day and weather.

![](./figures/routes.svg){width=45%}

## **Discrete or Continuous Optimization**

**Discrete** variables have only a finite number of possible values, whereas
**continuous** variables have an infinite number of possible values.

e.g.: If we are deciding in what order to attack a series of tasks on a list,
discrete optimization is employed.

![Continuous function](./figures/continuous.svg){width=45%}

## **Natural Optimization Methods**

- **Particle swarm optimization**         
  -- inspired by flocking birds/schooling fish           
- **Ant colony optimization**       
  -- inspired by the social behavior of ants      
- **Simulated Annealing**             
  -- inspired by the anealing process for material        
- **Genetic Algorithm**     
  -- inspired by the Darwinian evolution process     
- **_etc_**.

## **Particle Swarm Optimization - _PSO_**
PSO simulates the behavior of bird flocking.    
**_Suppose_**: a group of birds are randomly searching food in an
area. There is only one piece of food in the area being searched.
No bird knows where the food is.
But they know how far the food is in each iteration.
An effective way to find the food is to **follow the bird which is nearest to
the food.**

![](./figures/bird.svg){width=55%}

## **PSO**
PSO is a population based algorithm that uses a number of _particles_ which form
a swarm to search for an optimum.     

Each particle adjusts their flying based on their own and other particle's
experience until the swarm finally **finds the optimum** or the **search
budget is exhausted**.      

_Search budget_ could be based on time, number of iterations, etc.

## **PSO Algorithm** ##
1. Begin with a random population of particles _(solutions)_.     
2. For each particle, calculate the fitness score.      
  -- Fitness score shows the distance from the optimum                     
  -- If the fitness score is better than the value the row has ever had, update
  **pBest**.     
3. Choose the particle with the best fitness score.
4. For each particle, calculate the **velocity**, and update the **position** of
  the particle accordingly.      
5. repeat 2-4 until global optimum is found, or search budget is
   exhausted.       

## **How to calculate position and velocity?**
**$x$**^$i$^~$(k+1)$~ $=$ $x$^$i$^~$k$~ $+$ $v$^$i$^~$(k+1)$~             
**$v$**^$i$^~$(k+1)$~ $=$ $w$~$k$~**$v$**^$i$^~$k$~ $+$ $c$~$1$~$r$~$1$~ (**$p$**^$i$^~$k$~ $-$ **$x$**^$i$^~$k$~) $+$ $c$~$2$~$r$~$2$~ (**$p$**^$g$^~$k$~ $-$ **$x$**^$i$^~$k$~)                

. . .
     
**where:**               
**$p$**^$i$^~$(k)$~ $=$ best position particle $i$ has had                
**$p$**^$g$^~$(k)$~ $=$ best particle position        
$w$~$k$~ $=$ constant inertia weight       
$c$~$1$~, $c$~$2$~ $=$  learning factors (usually $c$~$1$~ $=$ $c$~$2$~ $=$ $2$)          
$r$~$1$~, $r$~$2$~ $=$  random numbers between $0$ and $1$      

## **Let's see an example!**    
   
## **More on PSO**   
For more on PSO, check out the following:  
              
 - https://ch.mathworks.com/help/gads/particle-swarm-optimization-algorithm.html     


## **Simulated Annealing**
Simulates the annealing process:     
![](./figures/simulated.svg){width=45%}

. . . 

a substance is heated above its melting temperature and then gradually cooled to
produce the crystalline lattice, which minimizes its energy probability
distribution.


## **Simulated Annealing**
So lowering the temperature until the material is in a steady _frozen_ state can
be preceived as searching for the material state in
which the internal particles have the minimum energy.

![](./figures/searchtemp.svg){width=40%}

## **Simulated Annealing (SA) Algorithm**
    
1. Set temperature **$T$** to maximum
2. Generate a random solution
3. Calculate fitness/cost
4. Generate a neighbouring solution
5. Calculate the fitness/cost of the new solution
6. Compare **$f$~(_new_)~** and **$f$~(_old_)~**      
  **if**  the new solution is better then take it!        
  **else** use an acceptance rate to choose between the two!       
7. Reset **$T$**:  **$T$** * **$alpha$**       
    (normally $0.8<$**$alpha$**$<0.99$)
8. Repeat 4-7 until:   
		either the global optimum is found  
		or **$T$** reaches the minimum.

## **Let's play a guessing game!**

![](./figures/guessing.svg){width=75%}


## **Guess implementation**
```{python , echo=TRUE}
class Guess:
    def __init__(self, chars, fitness):
        self.Chars = chars
        self.Fitness = fitness

def generate_guess(length, charSet, get_fitness):
    chars = []
    while len(chars) < length:
        sampleSize = min(length - len(chars), len(charSet))
        chars.extend(random.sample(charSet, sampleSize))
    chars = ''.join(chars)
    fitness = get_fitness(chars)
    return Guess(chars, fitness)
```
## **How to get a neighbor solution?**     

```{python , echo=TRUE}
def get_neighbor(parent, charSet, get_fitness):
    index = random.randrange(0, len(parent.Chars))
    childChars = list(parent.Chars)
    newChar, alternate = random.sample(charSet, 2)
    childChars[index] = alternate if newChar == childChars[index] else newChar
    chars = ''.join(childChars)
    fitness = get_fitness(chars)
    return Guess(chars, fitness)
```
    
## **Fitness function and acceptance rate**    

Reward a guess for every correct character in the right place!       
```{python , echo=TRUE}   
def get_fitness(guess, target):
    return sum(1 for expected, actual in zip(target, guess)
               if expected == actual)
```        
. . .      
   
As the temprature goes down (search progresses), the probability of
accepting a worse guess also goes down!   
```{python , eval=FALSE}
def acceptance_probability(prev_score, next_score, temperature):     
    return math.exp(temperature/(1+temperature))
``` 

## **Implementation of the annealing process**  

```{python , echo=TRUE}
def anneal(get_fitness, targetLen, optimalFitness, charSet, display, temperature, min_temp, alpha):
    bestGuess = generate_guess(targetLen, charSet, get_fitness)
    display(bestGuess)
    best_fitness = bestGuess.Fitness
    if bestGuess.Fitness >= optimalFitness:
        return bestGuess
    T = temperature
    T_min = min_temp
    while T > T_min:
        new_solution = get_neighbor(bestGuess, charSet, get_fitness)
        new_fitness = new_solution.Fitness
        if new_fitness == optimalFitness:
            return new_solution
        if new_fitness > best_fitness:
            bestGuess = new_solution
            best_fitness = new_fitness
        else:
            ar = acceptance_probability(best_fitness, new_fitness, T)
            if ar > random.uniform(2,3):
                bestGuess = new_solution
                best_fitness = new_fitness
        T = T*alpha
        display(bestGuess)
    return bestGuess
```
    

## **Let's now guess a password, using the SA algorithm!** ##

```{python , eval=FALSE}
class GuessPasswordTest(unittest.TestCase):
    charset = " abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!.,"

    def test_Hello_World(self):
        target = "Hello World!"
        self.guess_password(target)

    def guess_password(self, target):
        startTime = datetime.datetime.now()

        def fnGetFitness(chars):
            return sa.get_fitness(chars, target)

        def fnDisplay(candidate):
            sa.display(candidate, startTime)

        optimalFitness = len(target)
        best = sa.anneal(fnGetFitness, len(target), optimalFitness,
                                self.charset, fnDisplay, 100000.0, 0.00001, 0.99 )
        self.assertEqual(best.Chars, target)
```

## **Hill Climbing:**     
**a very similar optimziation algorithm to SA**

The difference is that worse neighbours will never get to be selected!

![](./figures/hillclimbing.svg){width=50%}

## **Local Optimum**

![](./figures/localoptimum.svg){width=55%}

. . .       

If hill climbing hits the local optimum, one approach is to restart the search!

## **Genetic Algorithms**
Inspired by the _Darwinian evolution_ and the concept of **survival of the
fittest**!
![](./figures/evolution.svg){width=75%}

## **Genetic Algorithm Overview**
Each point in the search space is an referred to as an _individual_
or _chromosome_.    

Individuals collectively form a _population_, which is _iteratively evolved_
until the optimum is found or the search budget is exhausted.

![](./figures/GA loop.svg){width=75%}

## **Evolutionary Operators in GA**      
    
**Crossover** is used to _re-combine_ the _properties_ from parents.    
**Mutation** is applied to explore the _neighborhood_!

![](./figures/operators.svg){width=75%}

## **Let's play the same guessing game!**

![](./figures/guessing.svg){width=75%}

## **What do we need to implement?**
1. Individual (_chromosome_) generator
2. Fitness function
3. Evolutionary operators (_only mutation for now_)
4. Search engine
5. Display function, showing the search progress (optional!) 

## **Individual Generator and Fitness Function**
```{python, eval=FALSE}
class Chromosome:
    def __init__(self, genes, fitness):
        self.Genes = genes
        self.Fitness = fitness
```
. . .     

```{python, eval=FALSE}
def generate_parent(length, geneSet, get_fitness):
    genes = []
    while len(genes) < length:
        sampleSize = min(length - len(genes), len(geneSet))
        genes.extend(random.sample(geneSet, sampleSize))
    genes = ''.join(genes)
    fitness = get_fitness(genes)
    return Chromosome(genes, fitness)
```
. . .     

```{python, eval=FALSE}  
def get_fitness(guess, target):
    return sum(1 for expected, actual in zip(target, guess)
               if expected == actual)
```

## **Mutation**

Similar to the one used in SA!

```{python, eval=FALSE}
def mutate(parent, geneSet, get_fitness):
    index = random.randrange(0, len(parent.Genes))
    childGenes = list(parent.Genes)
    newGene, alternate = random.sample(geneSet, 2)
    childGenes[index] = alternate if newGene == childGenes[index] 
                                  else newGene
    genes = ''.join(childGenes)
    fitness = get_fitness(genes)
    return Chromosome(genes, fitness)
```
. . .  

## **Search Engine**   

```{python, eval=FALSE}
def get_best(get_fitness, targetLen, optimalFitness, geneSet, display):
    random.seed()
    bestParent = _generate_parent(targetLen, geneSet, get_fitness)
    display(bestParent)
    if bestParent.Fitness >= optimalFitness:
        return bestParent
    while True:
        child = _mutate(bestParent, geneSet, get_fitness)
        if bestParent.Fitness >= child.Fitness:
            continue
        display(child)
        if child.Fitness >= optimalFitness:
            return child
        bestParent = child
```

## **Let's test it!**
```{python, eval=FALSE}
class GuessPasswordTest(unittest.TestCase):
    geneset = " abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!.,"

    def test_GA(self):
        target = "Hello World!"
        self.guess_password(target)

    def guess_password(self, target):
        startTime = datetime.datetime.now()

        def fnGetFitness(genes):
            return genetic.get_fitness(genes, target)

        def fnDisplay(candidate):
            genetic.display(candidate, startTime)

        optimalFitness = len(target)
        best = genetic.get_best(fnGetFitness, len(target), optimalFitness,
                                self.geneset, fnDisplay)
        self.assertEqual(best.Genes, target)
```

## What was missing in the implementation? ##
- implementation of selection
- implementation of crossover
- implementation of insertion

## **Travelling Salesman**

To further practice, try implementing GA for the TS problem!

![](./figures/salesman.svg){width=75%}

## **What are the _similarities_ and _differences_ among the algorithms?**

- individual representation
- fitness function
- random initialization
- local/global approaches

## **Which search algorithm to use?**
**No Free Lunch Theorem** says that the average performance of all search
algorithms on all optimization problems is equal!

_i.e._ GA performs no better than pure random search when applied to all
problems!

![](./figures/freelunch.svg){width=40%}

## **So again - which search algorithm to use?**
We can always start by simpler algorithms, e.g. hill climbing, and see if they
are good enough for the problem at hand.

If the search landscape is perturbated, more advanced algorithms which can
effectively explore the landscape may be a better start.


## **REFERENCES**

* The Python implementation is partially borrowed from:     
https://github.com/handcraftsman/GeneticAlgorithmsWithPython

* Details on _PSO_ is taken from:
http://www.swarmintelligence.org/tutorials.php
 

## Copyright
![Creative Commons](https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png) This
work is (c) 2017 - onwards by TU Delft and Mozhan Soltani and licensed under the
[Creative Commons Attribution-NonCommercial-ShareAlike 4.0
International](http://creativecommons.org/licenses/by-nc-sa/4.0/) license.

<!--html_preserve-->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-2510585-3', 'auto');
  ga('send', 'pageview');

</script>
<!--/html_preserve-->
